{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1599019449227",
   "display_name": "Python 3.7.6 64-bit ('deeplearning': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#3A913F;\">Handling Long Sequences</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Fighting the Unstable Gradients Problem</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the nature of RNN if we were to use a non saturating activation function and it so happened that the Gradient Descent updated the weights in a way that increased the outputs slighly at the first time step. Would result in the outputs exploding since the same weights are used at every time step. \n",
    "<br><br>\n",
    "Another common way method to mitigate the Unstable Gradients problem, for example BN does not inherently take into account the nature of RNN. As the same BN layer would be used at each time step, with the same parameters, regardless of the actual scale and offset of the inputs. \n",
    "<br><br>\n",
    "Another form of normalization called <em>Layer Normalization (LN)</em> often works much better. It normalized across the features dimension. An advantage to LN is that it compute the required statistics on the fly, at each time step, independently for each instance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LNSimpleRNNCell(keras.layers.Layer):\n",
    "    def __init__(self, units, activation = \"tanh\", **kwargs):\n",
    "        self.state_size = units\n",
    "        self.output_size = units\n",
    "        self.simple_rnn_cell = keras.layers.SimpleRNNCell(units,activation=None)\n",
    "    def call(self,inputs,states):\n",
    "        outputs, new_states = self.simple_rnn_cell(inputs,states)\n",
    "        norm_outputs = self.activation(self.layer_norm(outputs))\n",
    "        return norm_outputs, [norm_outputs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Tackling the Short-Term Memory Problem</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The LSTM cell can better detect long term dependencies. Along with learning what to store in the long-term state, what not to store, and what to read from it.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../jupyter_images/LSTM_cell.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#3A913F;\">Handling Long Sequences Summary:</h2>"
   ]
  }
 ]
}