{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(23)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Deep Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Would like to know how to avoid commmon problems associated with training deep neural networks? In this post we test our knowledge of how we can navigate around the problems associated with training deep neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some common issues/ things to keep in mind when training DNN are:\n",
    "* Vanishing/Exploding gradient problems in the lower layers of DNN\n",
    "* Optimizers to train efficiently\n",
    "* How to use regularization techniques to reduce the Risk of overfitting \n",
    "* Using unsupervised pretraining to tackle complex problems with little labeled data\n",
    "* How to reduce model traning time\n",
    "* How to leverage transfer learning i.e. using pretrained models lower layers to create a DNN to accomplish similar task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#3A913F;\">The Vanishing/ Exploding Gradients Problems</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe the Vanishing Gradients problem? What about the Exploding Gradients problem:\n",
    "<br><br>\n",
    "The Vanishing Gradient problem occurs when the gradient used to update each parameters weight (which is calculated from the gradient of the cost function with regard to each parameter in the network ) becomes increasinlgy smaller and smaller as the backpropagations algorithm progresses down to the lower layers. Resulting in the lower connection weights being left virtually unchanged, and consequently the training never converges to a good solution. On the other hand the backpropagation algorithm may experience the  exploding gradient problem where the gradients grow bigger and bigger until layers get absurdly large weight updates and the algorithm diverges. Both of this issues demonstrate the fact that DNN have naturally occuring unstable gradients; meaning different layers may learn at widely different speeds. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Properties of sigmoid activation function and initilization scheme of ~N(0,1) resulted in the variance of the outpus of each layer being much greater than the variance of its inputs. (AF has mean of 0.5 not 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Glorot and He Initialization</h2\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a layer what is fan-in and fan-out and how are they involved in Glorot initialization that solves the vanishing/exploding gradient problem (when using a sigmoid AF):\n",
    "<br><br>\n",
    "fan-in: is the number of inputs in the layer & fan-out: is the number of neurons in the layer. The Glorot initialization says that the <strong>connection weights in each layer must be initialized as:</strong>\n",
    "<br><br>\n",
    "N(0,$\\sigma^{2}$) where $\\sigma^{2}$ = $1/fan_{avg}$\n",
    "OR\n",
    "U~(-r,r) where r = $\\sqrt{\\frac{3}{fan_{avg}}}$\n",
    "<br>\n",
    "\n",
    "\n",
    "where $fan_{avg} = (fan_{in} +fan_{out})/2$\n",
    "\n",
    "\n",
    "How about for the RELU AF to avoid the V/E gradient problem what initialization strategy should you use?\n",
    "\n",
    "<br><br> Best to use the He initialization which only differs from the Glorot initialization by the scale of the variance i.e. using $fan_{in}$ where $\\sigma^{2} = \\frac{2}{fan_{in}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, Keras uses Glorot initialization with a uniform distribution. When creating\n",
    "a layer, you can change this to He initialization by setting kernel_initializer\n",
    "```python\n",
    "keras.layers.Dense(10, activation='relu',kernel_initialize=\"he_normal\")\n",
    "```\n",
    "Or you can also use the Variance Scaling initializer like this:\n",
    "\n",
    "```python\n",
    "he_avg_init = keras.initializers.VarianceScaling(scale=2., mode='fan_avg',\n",
    "distribution='uniform')\n",
    "keras.layers.Dense(10, activation=\"sigmoid\", kernel_initializer=he_avg_init)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Nonsaturating Activation Functions</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally the order in which Activation Functions should be checked are ...? Give a brief explanation of each AF e.g. characteristic and limitations. \n",
    "<br><br>\n",
    "SELU > ELU > leaky ReLU (and its variants) > ReLU > tanh \\> logistic\n",
    "<br><br>\n",
    "SELU: Scaled ELU tries to make the network <em> self normalize</em> that is for each output of each layer it will try to preserve a mean of 0 and standard deviation of 1 during training. But, it only works for sequential network achitectures,where each input features are standardized (mean 0 and standard deviation 1) and every hidden layer's weights must be initialized with LeCun normal initialization. \n",
    "<br><br>\n",
    "ELU: Alleviates the vanishing gradients problem as it facilitates an average output closer to 0 through allowing negative values when z $<$0. Avoids dead neurons problem with nonzero gradient when z <0. Convergence to a solution is supported by an $\\alpha$ value of 1 as the ELU AF will be smooth everywhere in effect speeding up Gradient DEscent. However, will be the ELU will be slow to compute due to its use of the exponential function.\n",
    "<br>\n",
    "$ ELU_a(z)=  \\left\\{\n",
    "\\begin{array}{ll}\n",
    "      \\alpha(exp(z)-1 &  z<0 \\\\\n",
    "      z & z\\ge0 \\\\\n",
    "\\end{array} \n",
    "\\right. $\n",
    "<br>\n",
    "Leaky ReLU: The hyperparameter α creates a small slope ensuring that the\n",
    "leaky ReLUs never die i.e. (output zero) when the input to it is negative. This can, in many cases, completely block backpropagation because the gradients will just be zero after one negative value has been inputted to the ReLU function;\n",
    "<br>\n",
    "$ LeakyReLU(z)= \\alpha x +x = \\left\\{\n",
    "\\begin{array}{ll}\n",
    "      z  & z>0 \\\\\n",
    "      \\alpha z & z\\ge0 \\\\\n",
    "\\end{array} \n",
    "\\right. $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Batch Normalization</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Normalization -> Provides support to minimize/elimanate the vanishing/exploading gradients problem that could occur during the training of a DNN. It effectively, learns the optimal scale and mean of each layer's inputs in a deep neural network.\n",
    "\n",
    "Here is the math behind batch normalization: the goal is zero center and normalize inputs and the algorithm accomplishes this by estimating each input's mean and standard deviation by evaluating the mean and standard deviation of teh input over the current mini batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../jupyter_images/Batch_Normalization_Algorithm.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape = [28,28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300,activation='elu',kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100,activation=\"elu\",kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10,activation=\"softmax\")  \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 271,346\n",
      "Trainable params: 268,978\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each BN layer adds four parameter per input γ, β, μ, and σ. The last two\n",
    "parameters, μ and σ, are the moving averages; they are not affected by backpropagation,\n",
    "so Keras calls them “non-trainable”9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235200"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "784*300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Normalization hyperparameters\n",
    "\n",
    "$\\hat{v}$ <- $\\hat{v} * momentum + v * (1-momentum)$ :Used to update the exponential moving averages; given a new value <strong>v</strong> (i.e., a new vector of input means or standard deviations computed over the current batch) typically value is close to 1 ~ .9, .99, .999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the task BN before the activation functionsadding the BN layers before the activation\n",
    "functions, rather than after (as we just did). There is some debate about this, as\n",
    "which is preferable seems to depend on the task—you can experiment with this too to\n",
    "see which option works best on your dataset. To add the BN layers before the activation\n",
    "functions, you must remove the activation function from the hidden layers and\n",
    "add them as separate layers after the BN layers. Moreover, since a Batch Normalization\n",
    "layer includes one offset parameter per input, you can remove the bias term from\n",
    "the previous layer (just pass use_bias=False when creating it):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape = [28,28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300,activation='elu',kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100,activation=\"elu\",kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10,activation=\"softmax\")  \n",
    "])\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "keras.layers.Flatten(input_shape=[28, 28]),\n",
    "keras.layers.BatchNormalization(),\n",
    "keras.layers.Dense(300, kernel_initializer=\"he_normal\", use_bias=False),\n",
    "keras.layers.BatchNormalization(),\n",
    "keras.layers.Activation(\"elu\"),\n",
    "keras.layers.Dense(100, kernel_initializer=\"he_normal\", use_bias=False),\n",
    "keras.layers.BatchNormalization(),\n",
    "keras.layers.Activation(\"elu\"),\n",
    "keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Gradient Clipping</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Clipping involves clipping the gradients during backpropagation so that they never exceed some threshold it is another technique to mitigate the exploding gradients problem.\n",
    "\n",
    "```python\n",
    "optimizer = keras.optimizer.SGD(clipvalue =1.0) or optimizer = keras.optimizer.SGD(clipnorm=1.0)\n",
    "#Given this optimizer, all partial derivatives of the loss (with regard to each and every trainable parameter) will be clipped if its l2 norm is greater than the threshold you picked.\n",
    "\n",
    "# clipnorm will preserve direction but not scale and vice versa for clipvalue.\n",
    "model.compile(loss=\"mse\",optimizer=optimizer)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#3A913F;\">Reusing Pretrained Layers</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that transfer learning will work best when the inputs have similar low level features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General Process for Transfer Learning:\n",
    "<br>\n",
    "- Freeze all the reused layers first (making their weights non-trainable so that Gradient Descent does not modify them) then train your model and see how it performs.\n",
    "- Then by sequence unfreeze one or two top hidden layers to let backpropagation modify them and see if performance improves. \n",
    "    - If you have a large set of training data you could unfreeze more hidden layers respectively.\n",
    "    - Keep in mind a small learning rate might be preferred so that you may preserve the finely tuned weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Transfer Learning with Keras</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "```python\n",
    "#clone model architecture and weights \n",
    "model_B = keras.models.clone_model(model_A)\n",
    "model_B.set_weights(model_A.get_weights()) \n",
    "\n",
    "#freeze layers weights \n",
    "for layer in model_B.layers[:-1]:\n",
    "    layer.trainable = False \n",
    "    \n",
    "#must compile after unfreezing or freezing layers \n",
    "model_B.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\",metrics=\"accuracy\") \n",
    "\n",
    "#rewire output connection in this particular case because it was randomized \n",
    "model_B.fit(X_train,Y_train, epochs=6, validation_data = (X_valid, Y_valid))\n",
    "\n",
    "#unfreeze \n",
    "for layer in model_B.layers[:-1]:\n",
    "    layer.trainable = True\n",
    "    \n",
    "#ensure we preserve the fine tuned weights; by using a decreased learning rate\n",
    "optimizer = keras.optimizers.SGD(lr=1e-4) #the default is le-2\n",
    "model_B.compile(loss=\"binary_crossentropy\",optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "history = model_B.fit(X_train, Y_train, epochs=16, validation_data=(X_valid, Y_valid))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing to keep in mind is that Transfer learning will learn shallow patters when trained on small dense networks and it will learn very specific patterns with dense networks. In either case those patterns may not be useful in other transfer tasks. <em>One architecture that benefits from Transfer learning is the Deep CNN because it tends to learn feature detectors that are general (especially in the lower layers)</em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Unsupervised Pretraining</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aerelien mentions that previously GBM and GAN were used for unsupervised pretraining these are Deep Learning techniques I am not currently familiar with. He also points out that generally GANs and autocoders are the techniques that are being used most commonly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Pretraining on an Auxillary Task</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used often in circumstances where you might not have readily available training data for your specific task but you do have data to train your lower layers so that they may be used for your limited training data for your specific task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#3A913F;\">Summary: Reusing Pretrained Layers</h1>\n",
    "There are multiple methods Transfer with different use cases. Transfer learning for speed and similar tasks, unsupervised pretraining for expensibe labeling of data, and pretraining on auxillary task to make use of readily available data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#3A913F;\">Faster Optimizers</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Momentum Optimization</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "An optimization of changing the weights associated with inputs, based on the in moment training loss from instance to instance. Simply put The gradients are updated not based on what the earlier graidents were but rather what the current local gradients are from instance to instance. That is it updates the weights by directly subtracting the gradient of cost functions with regard to the weights multiplied by the learning rate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Nesterov Accelerated Gradient</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than compute the gradients based on the local instance to instance or batch to batch feedback Nesterov Accelerated Gradient (NAG) computes the gradient in the direction of the momentum, theta + Bm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Adam and Nadam Optimization</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam <br>\n",
    "Keeps track of an exponentially decaying average of past gradients to optimize the moment (using the gradients from the most recent iterations and decaying this influence as you move away from these instances like a rolling window). It also keeps track of an exponentially decaying average of past squared gradients which are being used to decay the learning rate for steep dimensions and less so for dimensions for gentler slopes i.e an adaptive learning rate. \n",
    "<br><br>\n",
    "Nadam <br>\n",
    "This optimization techinique combines Adam with the Nesterov algorithm (updating weights in the directions of the momentum theta + Bm)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Learning Rate Scheduling</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test,y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "#Scaling the inputs to mean 0 and sd 1\n",
    "pixel_means = X_train.mean(axis=0,keepdims=True)\n",
    "pixel_stds = X_train.std(axis=0,keepdims=True)\n",
    "X_train_scaled = (X_train - pixel_means) / pixel_stds\n",
    "X_valid_scaled = (X_valid - pixel_means) / pixel_stds\n",
    "X_test_scaled = (X_test - pixel_means) / pixel_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneCycleScheduler(keras.callbacks.Callback):\n",
    "    def __init__(self, iterations,max_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Defining the Keras model to add callbacks to \n",
    "def get_model():\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dense(1, input_dim =784))\n",
    "    model.compile(\n",
    "        optimizer = keras.optimizers.RMSprop(learning_rate=0.1),\n",
    "        loss=\"mean_squared_error\",\n",
    "        metrics=[\"mean_absolute_error\"]\n",
    "    )\n",
    "    return model\n",
    "# Load example MNIST data and pre-process it\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape(-1, 784).astype(\"float32\") / 255.0\n",
    "x_test = x_test.reshape(-1, 784).astype(\"float32\") / 255.0\n",
    "\n",
    "# Limit the data to 1000 samples\n",
    "x_train = x_train[:1000]\n",
    "y_train = y_train[:1000]\n",
    "x_test = x_test[:1000]\n",
    "y_test = y_test[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCallback(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        if logs != None:\n",
    "            print(logs.keys())\n",
    "        # keys = list(logs.keys())\n",
    "        # print(\"Starting training; got log keys: {}\".format(keys))\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        if logs != None:\n",
    "            keys = list(logs.keys())\n",
    "            print(\"...Training: end of batch {}; got log keys: {}\".format\n",
    "            \n",
    "(batch, keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = get_model()\n",
    "# model.fit(\n",
    "#     x_train,\n",
    "#     y_train,\n",
    "#     batch_size=128,\n",
    "#     epochs=1,\n",
    "#     verbose=0,\n",
    "#     validation_split=0.5,\n",
    "#     callbacks=[CustomCallback()],\n",
    "# )\n",
    "\n",
    "# res = model.evaluate(\n",
    "#     x_test, y_test, batch_size=128, verbose=0, callbacks=[CustomCallback()]\n",
    "# )\n",
    "\n",
    "# res = model.predict(x_test, batch_size=128, callbacks=[CustomCallback()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#3A913F;\">Summary: Faster Optimizers</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaptive Optimizers of gradient descent are special. However, a 2017 paper by Ashia C. Wilson et al. showed that they can lead to solutions that generalize poorly on some datasets. So when you are disappointed by your model’s performance, try using plain Nesterov Accelerated Gradient instead: your dataset\n",
    "may just be allergic to adaptive gradients. In terms of learning rate scheduling you should consider using Performance scheduling in which you measure the validation error every N steps (just like for early stopping), and reduce the learning rate by a factor of λ when the error stops dropping. Or you can perfom 1cycle scheduling two mountains inverse of each other one being the momentum and the other being the learning rate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#3A913F;\">Avoiding Overfitting Through Regularization</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>ℓ1 and ℓ2 Regularization</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Dropout</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Monte Carlo (MC) Dropout</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Max-Norm Regularization</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#3A913F;\">Avoiding Overfitting Through Regularization Summary:</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#3A913F;\">Summary and Practical Guidelines</h1>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('deeplearning': conda)",
   "language": "python",
   "name": "python37764bitdeeplearningconda4784df4aecd74015b7cb859f264cd8e8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}