{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(23)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Deep Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Would like to know how to avoid commmon problems associated with training deep neural networks? In this post we test our knowledge of how we can navigate around the problems associated with training deep neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some common issues/ things to keep in mind when training DNN are:\n",
    "* Vanishing/Exploding gradient problems in the lower layers of DNN\n",
    "* Optimizers to train efficiently\n",
    "* How to use regularization techniques to reduce the Risk of overfitting \n",
    "* Using unsupervised pretraining to tackle complex problems with little labeled data\n",
    "* How to reduce model traning time\n",
    "* How to leverage transfer learning i.e. using pretrained models lower layers to create a DNN to accomplish similar task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#3A913F;\">The Vanishing/ Exploding Gradients Problems</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe the Vanishing Gradients problem? What about the Exploding Gradients problem:\n",
    "<br><br>\n",
    "The Vanishing Gradient problem occurs when the gradient used to update each parameters weight (which is calculated from the gradient of the cost function with regard to each parameter in the network ) becomes increasinlgy smaller and smaller as the backpropagations algorithm progresses down to the lower layers. Resulting in the lower connection weights being left virtually unchanged, and consequently the training never converges to a good solution. On the other hand the backpropagation algorithm may experience the  exploding gradient problem where the gradients grow bigger and bigger until layers get absurdly large weight updates and the algorithm diverges. Both of this issues demonstrate the fact that DNN have naturally occuring unstable gradients; meaning different layers may learn at widely different speeds. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Properties of sigmoid activation function and initilization scheme of ~N(0,1) resulted in the variance of the outpus of each layer being much greater than the variance of its inputs. (AF has mean of 0.5 not 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Glorot and He Initialization</h2\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a layer what is fan-in and fan-out and how are they involved in Glorot initialization that solves the vanishing/exploding gradient problem (when using a sigmoid AF):\n",
    "<br><br>\n",
    "fan-in: is the number of inputs in the layer & fan-out: is the number of neurons in the layer. The Glorot initialization says that the <strong>connection weights in each layer must be initialized as:</strong>\n",
    "<br><br>\n",
    "N(0,$\\sigma^{2}$) where $\\sigma^{2}$ = $1/fan_{avg}$\n",
    "OR\n",
    "U~(-r,r) where r = $\\sqrt{\\frac{3}{fan_{avg}}}$\n",
    "<br>\n",
    "\n",
    "\n",
    "where $fan_{avg} = (fan_{in} +fan_{out})/2$\n",
    "\n",
    "\n",
    "How about for the RELU AF to avoid the V/E gradient problem what initialization strategy should you use?\n",
    "\n",
    "<br><br> Best to use the He initialization which only differs from the Glorot initialization by the scale of the variance i.e. using $fan_{in}$ where $\\sigma^{2} = \\frac{2}{fan_{in}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, Keras uses Glorot initialization with a uniform distribution. When creating\n",
    "a layer, you can change this to He initialization by setting kernel_initializer\n",
    "```python\n",
    "keras.layers.Dense(10, activation='relu',kernel_initialize=\"he_normal\")\n",
    "```\n",
    "Or you can also use the Variance Scaling initializer like this:\n",
    "\n",
    "```python\n",
    "he_avg_init = keras.initializers.VarianceScaling(scale=2., mode='fan_avg',\n",
    "distribution='uniform')\n",
    "keras.layers.Dense(10, activation=\"sigmoid\", kernel_initializer=he_avg_init)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Nonsaturating Activation Functions</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally the order in which Activation Functions should be checked are ...? Give a brief explanation of each AF e.g. characteristic and limitations. \n",
    "<br><br>\n",
    "SELU > ELU > leaky ReLU (and its variants) > ReLU > tanh \\> logistic\n",
    "<br><br>\n",
    "SELU: Scaled ELU tries to make the network <em> self normalize</em> that is for each output of each layer it will try to preserve a mean of 0 and standard deviation of 1 during training. But, it only works for sequential network achitectures,where each input features are standardized (mean 0 and standard deviation 1) and every hidden layer's weights must be initialized with LeCun normal initialization. \n",
    "<br><br>\n",
    "ELU: Alleviates the vanishing gradients problem as it facilitates an average output closer to 0 through allowing negative values when z $<$0. Avoids dead neurons problem with nonzero gradient when z <0. Convergence to a solution is supported by an $\\alpha$ value of 1 as the ELU AF will be smooth everywhere in effect speeding up Gradient DEscent. However, will be the ELU will be slow to compute due to its use of the exponential function.\n",
    "<br>\n",
    "$ ELU_a(z)=  \\left\\{\n",
    "\\begin{array}{ll}\n",
    "      \\alpha(exp(z)-1 &  z<0 \\\\\n",
    "      z & z\\ge0 \\\\\n",
    "\\end{array} \n",
    "\\right. $\n",
    "<br>\n",
    "Leaky ReLU: The hyperparameter α creates a small slope ensuring that the\n",
    "leaky ReLUs never die i.e. (output zero) when the input to it is negative. This can, in many cases, completely block backpropagation because the gradients will just be zero after one negative value has been inputted to the ReLU function;\n",
    "<br>\n",
    "$ LeakyReLU(z)= \\alpha x +x = \\left\\{\n",
    "\\begin{array}{ll}\n",
    "      z  & z>0 \\\\\n",
    "      \\alpha z & z\\ge0 \\\\\n",
    "\\end{array} \n",
    "\\right. $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Batch Normalization</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Normalization -> Provides support to minimize/elimanate the vanishing/exploading gradients problem that could occur during the training of a DNN. It effectively, learns the optimal scale and mean of each layer's inputs in a deep neural network.\n",
    "\n",
    "Here is the math behind batch normalization: the goal is zero center and normalize inputs and the algorithm accomplishes this by estimating each input's mean and standard deviation by evaluating the mean and standard deviation of teh input over the current mini batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../jupyter_images/Batch_Normalization_Algorithm.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape = [28,28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300,activation='elu',kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100,activation=\"elu\",kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10,activation=\"softmax\")  \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 271,346\n",
      "Trainable params: 268,978\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each BN layer adds four parameter per input γ, β, μ, and σ. The last two\n",
    "parameters, μ and σ, are the moving averages; they are not affected by backpropagation,\n",
    "so Keras calls them “non-trainable”9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235200"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "784*300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Normalization hyperparameters\n",
    "\n",
    "$\\hat{v}$ <- $\\hat{v} * momentum + v * (1-momentum)$ :Used to update the exponential moving averages; given a new value <strong>v</strong> (i.e., a new vector of input means or standard deviations computed over the current batch) typically value is close to 1 ~ .9, .99, .999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Gradient Clipping</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Clipping involves clipping the gradients during backpropagation so that they never exceed some threshold it is another technique to mitigate the exploding gradients problem.\n",
    "\n",
    "```python\n",
    "optimizer = keras.optimizer.SGD(clipvalue =1.0) or optimizer = keras.optimizer.SGD(clipnorm=1.0)\n",
    "#Given this optimizer, all partial derivatives of the loss (with regard to each and every trainable parameter) will be clipped if its l2 norm is greater than the threshold you picked.\n",
    "\n",
    "# clipnorm will preserve direction but not scale and vice versa for clipvalue.\n",
    "model.compile(loss=\"mse\",optimizer=optimizer)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#3A913F;\">Reusing Pretrained Layers</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that transfer learning will work best when the inputs have similar low level features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General Process for Transfer Learning:\n",
    "<br>\n",
    "- Freeze all the reused layers first (making their weights non-trainable so that Gradient Descent does not modify them) then train your model and see how it performs.\n",
    "- Then by sequence unfreeze one or two top hidden layers to let backpropagation modify them and see if performance improves. \n",
    "    - If you have a large set of training data you could unfreeze more hidden layers respectively.\n",
    "    - Keep in mind a small learning rate might be preferred so that you may preserve the finely tuned weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Transfer Learning with Keras</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "```python\n",
    "#clone model architecture and weights \n",
    "model_B = keras.models.clone_model(model_A)\n",
    "model_B.set_weights(model_A.get_weights()) \n",
    "\n",
    "#freeze layers weights \n",
    "for layer in model_B.layers[:-1]:\n",
    "    layer.trainable = False \n",
    "    \n",
    "#must compile after unfreezing or freezing layers \n",
    "model_B.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\",metrics=\"accuracy\") \n",
    "\n",
    "#rewire output connection in this particular case because it was randomized \n",
    "model_B.fit(X_train,Y_train, epochs=6, validation_data = (X_valid, Y_valid))\n",
    "\n",
    "#unfreeze \n",
    "for layer in model_B.layers[:-1]:\n",
    "    layer.trainable = True\n",
    "    \n",
    "#ensure we preserve the fine tuned weights; by using a decreased learning rate\n",
    "optimizer = keras.optimizers.SGD(lr=1e-4) #the default is le-2\n",
    "model_B.compile(loss=\"binary_crossentropy\",optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "history = model_B.fit(X_train, Y_train, epochs=16, validation_data=(X_valid, Y_valid))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing to keep in mind is that Transfer learning will learn shallow patters when trained on small dense networks and it will learn very specific patterns with dense networks. In either case those patterns may not be useful in other transfer tasks. <em>One architecture that benefits from Transfer learning is the Deep CNN because it tends to learn feature detectors that are general (especially in the lower layers)</em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Unsupervised Pretraining</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Pretraining on an Auxillary Task</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#3A913F;\">Summary: Reusing Pretrained Layers</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#3A913F;\">Faster Optimizers</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#3A913F;\">Avoiding Overfitting Through Regularization</h1>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('deeplearning': conda)",
   "language": "python",
   "name": "python37764bitdeeplearningconda4784df4aecd74015b7cb859f264cd8e8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
