{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION 3: Why is it generally preferable to use a Logistic Regression classifier rather than a classical Perceptron (i.e., a single layer of threshold logic units trained using the Perceptron training algorithm)? How can you tweak a Perceptron to make it equivalent to a Logistic Regression classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that to predict/classify something we are modeling inputs to outputs with a function(s). And in the case of a logistic regression classifier, it uses the sigmoid function to bound outputs(probability) from 0 to 1 in terms of inputs. Versus the classical Perceptron with a single TLU, models outputs in terms of inputs using a hard threshold (Heaviside step function).Additionally, in the classical Perceptron decision boundaries for each output neuron are linear and will converge to a solution if training instances are linearly separable. while on the other hand, Logistic Regression decision boundaries are based on the sigmoid function, making it more flexible and allowing it to converge to an approximate solution to a classification task. Thus, Why Logistic is generally preferable over classical Perceptron. However, you can stack multiple Perceptrons to get an ANN architecture known as a Multilayer Perceptron which can be trained using some optimization algorithm minimizing the cost function. This along with the logistic(sigmoid) function as opposed to the step function for the activation function will make an ANN architecture equivalent to a Logistic Regression classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION 4: Why was the logistic activation function a key ingredient in training the first MLPs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceptron learning rule reinforces connections that help reduce the error. More specifically, the Perceptron is fed one training instance at a time, and for each instance it makes its predictions. For every output neuron that produced a wrong prediction, it reinforces the connection weights from the inputs that would have contributed to the correct prediction. The derivative of the logistic activation function is always nonzero, allowing Gradient Descent to roll down the slope. When the activation is a step function gradient descent cannot move as there is not slope at all. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION 5: Name three popular activation functions. Can you draw them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RELU, sigmoind, Hyperbolic tangent funciton: \n",
    "\n",
    "\n",
    "<p><strong>Sigmoid functions</strong>: mostly used in in shallow networks , binary classification problems<br>\n",
    "    <li>Main advantages of the sigmoid functions - Being easy to understand (probability)</li>\n",
    "<br>\n",
    "<li>Sigmoid AF suffers major drawbacks which include sharp damp gradients during backpropagation from deeper hidden layers to the input layers, gradient saturation ( gradient/derivative value is very small. Consequently, after numerous iterations the weights get updated so slowly because the value of gradient is so much small), slow convergence and non-zero centred output thereby causing the gradient\n",
    "updates to propagate in different directions</li>\n",
    "\n",
    "\n",
    "<p><strong>Tanh function</strong>: became the preferred function compared to the sigmoid function in that it gives better training performance for multi-layer neural networks by remedy of drawbacks from sigmoid AF. However, the tanh function could not solve the vanishing gradient problem suffered by the sigmoid functions as well.\n",
    "    <br>\n",
    "    <li>The main advantage provided by the function is that it produces zero centred output thereby aiding the back-propagation process.</li></p>\n",
    "\n",
    "\n",
    "<p><h3>Finally the Rectified Linear Unit (ReLU) Function:</h3><br>It offers the better performance and generalization in deep learning compared to the Sigmoid and Tanh activation function.\n",
    "<br>\n",
    "    \n",
    "<h3>Advantages</h3>\n",
    "<li>The ReLU represents a nearly linear function and therefore preserves the properties of linear models that made them easy to optimize, with the gradient-descent method.</li>\n",
    "<br>\n",
    "<li> Eliminates the vanishing gradient problem by rectifying the values of the inputs less than zero and forcing them to zero.</li>\n",
    "    \n",
    "<li> Guarantee faster computation since it does not compute exponentials and divisions, with overall speed of computation enhanced</li>\n",
    "\n",
    "<h3>Limitations</h3>\n",
    "ReLU has alimitation that it easily overfits compared to the sigmoid function although the dropout technique has been adopted to reduce\n",
    "the effect of overfitting of ReLUs\n",
    "\n",
    "Sometimes fragile during training thereby causing some of the gradients to die. This leads to some neurons being dead as well, thereby causing the weight updates not to activate in future data points,\n",
    "thereby hindering learning as dead neurons gives zero activation. hink about the chain rule in the backward pass. If the derivative of the slope of the ReLU is of 0, absolutely no learning is performed on the layers below the dead ReLU, because 0 will be multiplied to the accumulated gradient for the weight update. \n",
    "\n",
    "<h3>Solution?</h3>\n",
    "To resolve the dead neuron issues, the leaky ReLU was\n",
    "proposed with a small negative slope to the ReLU\n",
    "to sustain and keep the weight updates alive during the entire propagation process.\n",
    "\n",
    "\n",
    "But, The LReLU has an identical result when compared to the standard ReLU. Perhaps because the ReLU can introduce a sort of “optimal brain damage” regularization in your machine learning algorithm thus avoiding overfitting and improving genrelaization.\n",
    "\n",
    "Activation Functions: Comparison of Trends in Practice and Research for Deep Learning[https://arxiv.org/pdf/1811.03378.pdf] <br>\n",
    "https://www.quora.com/What-are-the-advantages-of-using-Leaky-Rectified-Linear-Units-Leaky-ReLU-over-normal-ReLU-in-deep-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "What is backpropagation and how does it work? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
